{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5099ad5a",
   "metadata": {},
   "source": [
    "# Data Analysis \n",
    "MRBench_V1: The original dataset with 192 dialogues as deatiled in the paper.\n",
    "\n",
    "MRBench_V2: An updated version with additional 8 dialogues, bringing the total to 200 examples.\n",
    "\n",
    "\n",
    "conduct DAMR / Annotation correlation (AC) scores for original result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54c77316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "454813c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_MRBench_response(data, model_name):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for i, dialogue in enumerate(data):\n",
    "        print(\"*\"*100)\n",
    "        print(f\"Dialogue {i+1}:\")\n",
    "        print(f\"  Data: {dialogue['Data']}\")\n",
    "        print(f\"  Topic: {dialogue['Topic']}\")\n",
    "        # print(f\"  Conversation History: {dialogue['conversation_history']}\")\n",
    "        print(f\"  >>>>Ground Truth Solution: {dialogue['Ground_Truth_Solution']}\")\n",
    "        llm_model_response = dialogue['anno_llm_responses'][model_name]['response']\n",
    "        print(f\"  >>>>LLM Response: {llm_model_response}\")\n",
    "        print(\"*\"*100)\n",
    "\n",
    "def print_MRBench_label(data, model_name, annotation_name):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    from collections import defaultdict\n",
    "    label_dict = defaultdict(int)\n",
    "    for i, dialogue in enumerate(data):\n",
    "        print(\"*\"*100)\n",
    "        print(f\"Dialogue {i+1}:\")\n",
    "        print(f\"  Data: {dialogue['Data']}\")\n",
    "        # if dianuelogue['Data'] == \"MathDial\":\n",
    "        #     conti\n",
    "        # print(f\"  Topic: {dialogue['Topic']}\")\n",
    "        # print(f\"  Conversation History: {dialogue['conversation_history']}\")\n",
    "        # print(f\"  >>>>Ground Truth Solution: {dialogue['Ground_Truth_Solution']}\")\n",
    "        annotation = dialogue['anno_llm_responses'][model_name]['annotation'][annotation_name]\n",
    "        label_dict[annotation] += 1\n",
    "        print(f\"  >>>>{annotation_name}: {annotation}\")\n",
    "    print(label_dict)\n",
    "    # label_dict  Yes / All\n",
    "    for k, v in label_dict.items():\n",
    "        print(f\"{k}: {v/len(data)*100}%\")\n",
    "# MRBenchv1_data = json.load(open(\"../data/MRBench/MRBench_V2.json\"))\n",
    "# print_MRBench_data(MRBenchv1_data, \"Mistral\")\n",
    "# print_MRBench_label(MRBenchv1_data, \"Gemini\", \"Tutor_Tone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5227024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation_label(key, label):\n",
    "    label = label.lower().strip()\n",
    "    Tutor_tone_mapping = {\n",
    "        \"encouraging\": 1,\n",
    "        \"neutral\": 2,\n",
    "        \"offensive\": 3\n",
    "    }\n",
    "    \n",
    "    Other_rule_mapping = {\n",
    "        \"yes\": 1,\n",
    "        \"to some extent\": 2,\n",
    "        \"no\": 3\n",
    "    }\n",
    "    def map_revealing_of_the_answer(label):\n",
    "        label = label.lower().strip()\n",
    "        if label.startswith(\"yes\") and \"correct\" in label:\n",
    "            return 1\n",
    "        elif label.startswith(\"yes\") and \"incorrect\" in label:\n",
    "            return 2\n",
    "        elif label.startswith(\"no\"):\n",
    "            return 3\n",
    "        else:\n",
    "            return None\n",
    "    if key == \"Revealing_of_the_Answer\":\n",
    "        return map_revealing_of_the_answer(label)\n",
    "    else:\n",
    "        map_dict = Tutor_tone_mapping if key == \"Tutor_Tone\" else Other_rule_mapping\n",
    "        for key, value in map_dict.items():\n",
    "            if label.startswith(key):\n",
    "                return value\n",
    "        print(label)\n",
    "        return None\n",
    "\n",
    "\n",
    "desiderata = {\n",
    "    \"Mistake_Identification\": 1,  # Yes\n",
    "    \"Mistake_Location\": 1,        # Yes\n",
    "    \"Revealing_of_the_Answer\": 3,        # No\n",
    "    \"Providing_Guidance\": 1,      # Yes\n",
    "    \"Actionability\": 1,           # Yes\n",
    "    \"Coherence\": 1,                # Yes\n",
    "    \"Tutor_Tone\": 1,              # Encouraging\n",
    "    \"humanlikeness\": 1,               # Yes\n",
    "}\n",
    "\n",
    "# new_annotation\n",
    "def evaluate_ordinary_desiderata(eval_data, data_type=\"All\", verbose=False):\n",
    "    from collections import defaultdict\n",
    "    evaluation_result = defaultdict(dict)\n",
    "    for data in eval_data:\n",
    "        d_type= data['Data']\n",
    "        if data_type !=\"All\" and data_type != d_type:\n",
    "            print(f\"Skip {d_type}\")\n",
    "            continue\n",
    "        for model, value in data['anno_llm_responses'].items():\n",
    "            annotation_point = value['annotation_point']\n",
    "            for k, v in annotation_point.items(): \n",
    "                if v is None:\n",
    "                    if verbose:\n",
    "                        print(model, k, v)\n",
    "                    continue\n",
    "                if v == desiderata[k]:\n",
    "                    if k not in evaluation_result[model]:\n",
    "                        evaluation_result[model][k] = [0,0]\n",
    "                    evaluation_result[model][k][0] += 1\n",
    "                else:\n",
    "                    if k not in evaluation_result[model] and v is not None:\n",
    "                        evaluation_result[model][k] = [0,0]\n",
    "                    evaluation_result[model][k][1] += 1\n",
    "    return evaluation_result\n",
    "def print_evaluation_result(evaluation_result):\n",
    "    import pandas as pd\n",
    "    pd_result = []\n",
    "    columns = []\n",
    "    for model, value in evaluation_result.items():\n",
    "        model_result = []\n",
    "        value = sorted(value.items(), key=lambda x: x[0])\n",
    "        columns = [k for k, v in value]\n",
    "        for k, v in value:\n",
    "            # print(v[0], v[1]+v[0])\n",
    "            model_result.append((v[0]/(v[0]+v[1] )* 100.0))\n",
    "        pd_result.append([model] + model_result)  \n",
    "    columns = ['Tutor'] + columns\n",
    "    pd_result = pd.DataFrame(pd_result, columns=columns)\n",
    "    columns_mapping = {\n",
    "        'Mistake_Identification': 'Mistake_Identification',\n",
    "        'Mistake_Location': 'Mistake_Location',\n",
    "        'Revealing_of_the_Answer': 'Revealing_of_the_Answer',\n",
    "        'Providing_Guidance': 'Providing_Guidance',\n",
    "        'Actionability': 'Actionability',\n",
    "        'Coherence': 'Coherence',\n",
    "        'Tutor_Tone': 'Tutor_Tone',\n",
    "        'humanlikeness': 'Human-likeness'\n",
    "    }\n",
    "    pd_result.rename(columns=columns_mapping, inplace=True)\n",
    "    pd_result = pd_result[['Tutor', 'Mistake_Identification', 'Mistake_Location', 'Revealing_of_the_Answer', 'Providing_Guidance', 'Actionability', 'Coherence', 'Tutor_Tone', 'Human-likeness']].round(2)\n",
    "    return pd_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ecddba",
   "metadata": {},
   "source": [
    "# Data Analysis \n",
    "MRBench_V1: The original dataset with 192 dialogues as deatiled in the paper.\n",
    "\n",
    "MRBench_V2: An updated version with additional 8 dialogues, bringing the total to 200 examples.\n",
    "\n",
    "\n",
    "conduct DAMR score for original result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e8bb0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f2b5b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dialogues in MRBenchv1 is  192\n",
      "Number of dialogues in MRBenchv2 is  200\n",
      "The maximum length of the ground truth solution in MRBenchv1 is  579\n",
      "MRBenchv1 - The number of dialogues annotated by  Gemini is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Phi3 is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama318B is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama31405B is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Mistral is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Expert is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  GPT4 is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Sonnet is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Novice is  53\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MRBenchv1 - The number of dialogues annotated by  Gemini_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Phi3_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama318B_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama31405B_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Mistral_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Expert_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  GPT4_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Sonnet_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama31405B_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Mistral_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  GPT4_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama318B_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Novice_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Phi3_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Expert_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Sonnet_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Gemini_Bridge is  53\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MRBenchv2 - The number of dialogues annotated by  Gemini is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Phi3 is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Llama318B is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Llama31405B is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Mistral is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Expert is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  GPT4 is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Sonnet is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Novice is  55\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "root_dir = \"../data\"\n",
    "MRBenchv1 = os.path.join(root_dir, \"MRBench/MRBench_V1.json\")\n",
    "MRBenchv2 = os.path.join(root_dir, \"MRBench/MRBench_V2.json\")\n",
    "MRBenchv1_data = json.load(open(MRBenchv1))\n",
    "MRBenchv2_data = json.load(open(MRBenchv2))\n",
    "print(\"Number of dialogues in MRBenchv1 is \", len(MRBenchv1_data))\n",
    "print(\"Number of dialogues in MRBenchv2 is \", len(MRBenchv2_data))\n",
    "# MRBenchv1_data[0]\n",
    "# count the length of the ground truth solution\n",
    "length_list = [len(data['Ground_Truth_Solution']) for data in MRBenchv1_data]\n",
    "print(\"The maximum length of the ground truth solution in MRBenchv1 is \", max(length_list))\n",
    "\n",
    "model_count = defaultdict(int)\n",
    "model_data_count = defaultdict(int)\n",
    "for data in MRBenchv1_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        model_count[key] += 1\n",
    "        model_data_count[key + \"_\" + data['Data']] += 1\n",
    "for k, v in model_count.items():\n",
    "    print(\"MRBenchv1 - The number of dialogues annotated by \", k, \"is \", v)\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k, v in model_data_count.items():\n",
    "    print(\"MRBenchv1 - The number of dialogues annotated by \", k, \"is \", v)\n",
    "print(\"-\"*100)\n",
    "model_count = defaultdict(int)\n",
    "for data in MRBenchv2_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        model_count[key] += 1\n",
    "for k, v in model_count.items():\n",
    "    print(\"MRBenchv2 - The number of dialogues annotated by \", k, \"is \", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be341f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tutor</th>\n",
       "      <th>Mistake_Identification</th>\n",
       "      <th>Mistake_Location</th>\n",
       "      <th>Revealing_of_the_Answer</th>\n",
       "      <th>Providing_Guidance</th>\n",
       "      <th>Actionability</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Tutor_Tone</th>\n",
       "      <th>Human-likeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expert</td>\n",
       "      <td>81.25</td>\n",
       "      <td>68.75</td>\n",
       "      <td>97.92</td>\n",
       "      <td>72.92</td>\n",
       "      <td>81.77</td>\n",
       "      <td>84.90</td>\n",
       "      <td>17.19</td>\n",
       "      <td>94.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Expert_paper</td>\n",
       "      <td>76.04</td>\n",
       "      <td>63.02</td>\n",
       "      <td>90.62</td>\n",
       "      <td>67.19</td>\n",
       "      <td>76.04</td>\n",
       "      <td>79.17</td>\n",
       "      <td>92.19</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT-4_paper</td>\n",
       "      <td>94.27</td>\n",
       "      <td>84.38</td>\n",
       "      <td>53.12</td>\n",
       "      <td>76.04</td>\n",
       "      <td>46.35</td>\n",
       "      <td>90.17</td>\n",
       "      <td>37.50</td>\n",
       "      <td>89.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT4</td>\n",
       "      <td>94.27</td>\n",
       "      <td>85.42</td>\n",
       "      <td>54.69</td>\n",
       "      <td>77.08</td>\n",
       "      <td>46.88</td>\n",
       "      <td>92.71</td>\n",
       "      <td>36.98</td>\n",
       "      <td>93.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>87.50</td>\n",
       "      <td>62.50</td>\n",
       "      <td>92.71</td>\n",
       "      <td>58.85</td>\n",
       "      <td>61.98</td>\n",
       "      <td>82.29</td>\n",
       "      <td>39.58</td>\n",
       "      <td>95.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini_paper</td>\n",
       "      <td>63.02</td>\n",
       "      <td>39.58</td>\n",
       "      <td>67.71</td>\n",
       "      <td>37.50</td>\n",
       "      <td>42.71</td>\n",
       "      <td>56.77</td>\n",
       "      <td>21.88</td>\n",
       "      <td>68.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>95.31</td>\n",
       "      <td>84.90</td>\n",
       "      <td>81.77</td>\n",
       "      <td>77.60</td>\n",
       "      <td>75.52</td>\n",
       "      <td>94.27</td>\n",
       "      <td>17.71</td>\n",
       "      <td>93.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama31405B_paper</td>\n",
       "      <td>94.27</td>\n",
       "      <td>84.38</td>\n",
       "      <td>80.73</td>\n",
       "      <td>77.08</td>\n",
       "      <td>74.48</td>\n",
       "      <td>91.67</td>\n",
       "      <td>16.15</td>\n",
       "      <td>90.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama318B</td>\n",
       "      <td>81.25</td>\n",
       "      <td>56.25</td>\n",
       "      <td>76.56</td>\n",
       "      <td>46.88</td>\n",
       "      <td>42.71</td>\n",
       "      <td>82.81</td>\n",
       "      <td>19.79</td>\n",
       "      <td>96.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama318B_paper</td>\n",
       "      <td>80.21</td>\n",
       "      <td>54.69</td>\n",
       "      <td>73.96</td>\n",
       "      <td>45.31</td>\n",
       "      <td>42.71</td>\n",
       "      <td>80.73</td>\n",
       "      <td>19.79</td>\n",
       "      <td>93.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>93.23</td>\n",
       "      <td>74.48</td>\n",
       "      <td>89.06</td>\n",
       "      <td>66.15</td>\n",
       "      <td>71.35</td>\n",
       "      <td>88.02</td>\n",
       "      <td>16.67</td>\n",
       "      <td>97.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mistral_paper</td>\n",
       "      <td>93.23</td>\n",
       "      <td>73.44</td>\n",
       "      <td>86.46</td>\n",
       "      <td>63.54</td>\n",
       "      <td>70.31</td>\n",
       "      <td>86.98</td>\n",
       "      <td>15.10</td>\n",
       "      <td>95.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Novice</td>\n",
       "      <td>49.06</td>\n",
       "      <td>16.98</td>\n",
       "      <td>88.68</td>\n",
       "      <td>13.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>56.60</td>\n",
       "      <td>54.72</td>\n",
       "      <td>37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Novice_paper</td>\n",
       "      <td>43.33</td>\n",
       "      <td>16.67</td>\n",
       "      <td>80.00</td>\n",
       "      <td>11.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phi3</td>\n",
       "      <td>28.65</td>\n",
       "      <td>26.56</td>\n",
       "      <td>79.17</td>\n",
       "      <td>18.23</td>\n",
       "      <td>11.46</td>\n",
       "      <td>38.54</td>\n",
       "      <td>47.40</td>\n",
       "      <td>52.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phi3_paper</td>\n",
       "      <td>28.65</td>\n",
       "      <td>26.04</td>\n",
       "      <td>73.96</td>\n",
       "      <td>17.71</td>\n",
       "      <td>11.98</td>\n",
       "      <td>39.58</td>\n",
       "      <td>45.31</td>\n",
       "      <td>52.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sonnet</td>\n",
       "      <td>86.98</td>\n",
       "      <td>71.35</td>\n",
       "      <td>96.88</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.50</td>\n",
       "      <td>90.62</td>\n",
       "      <td>57.81</td>\n",
       "      <td>98.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sonnet_paper</td>\n",
       "      <td>85.42</td>\n",
       "      <td>69.79</td>\n",
       "      <td>94.79</td>\n",
       "      <td>59.38</td>\n",
       "      <td>60.94</td>\n",
       "      <td>88.54</td>\n",
       "      <td>54.69</td>\n",
       "      <td>96.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tutor  Mistake_Identification  Mistake_Location  \\\n",
       "5             Expert                   81.25             68.75   \n",
       "1       Expert_paper                   76.04             63.02   \n",
       "7        GPT-4_paper                   94.27             84.38   \n",
       "6               GPT4                   94.27             85.42   \n",
       "0             Gemini                   87.50             62.50   \n",
       "4       Gemini_paper                   63.02             39.58   \n",
       "3        Llama31405B                   95.31             84.90   \n",
       "8  Llama31405B_paper                   94.27             84.38   \n",
       "2          Llama318B                   81.25             56.25   \n",
       "2    Llama318B_paper                   80.21             54.69   \n",
       "4            Mistral                   93.23             74.48   \n",
       "6      Mistral_paper                   93.23             73.44   \n",
       "8             Novice                   49.06             16.98   \n",
       "0       Novice_paper                   43.33             16.67   \n",
       "1               Phi3                   28.65             26.56   \n",
       "3         Phi3_paper                   28.65             26.04   \n",
       "7             Sonnet                   86.98             71.35   \n",
       "5       Sonnet_paper                   85.42             69.79   \n",
       "\n",
       "   Revealing_of_the_Answer  Providing_Guidance  Actionability  Coherence  \\\n",
       "5                    97.92               72.92          81.77      84.90   \n",
       "1                    90.62               67.19          76.04      79.17   \n",
       "7                    53.12               76.04          46.35      90.17   \n",
       "6                    54.69               77.08          46.88      92.71   \n",
       "0                    92.71               58.85          61.98      82.29   \n",
       "4                    67.71               37.50          42.71      56.77   \n",
       "3                    81.77               77.60          75.52      94.27   \n",
       "8                    80.73               77.08          74.48      91.67   \n",
       "2                    76.56               46.88          42.71      82.81   \n",
       "2                    73.96               45.31          42.71      80.73   \n",
       "4                    89.06               66.15          71.35      88.02   \n",
       "6                    86.46               63.54          70.31      86.98   \n",
       "8                    88.68               13.21           1.89      56.60   \n",
       "0                    80.00               11.67           1.67      50.00   \n",
       "1                    79.17               18.23          11.46      38.54   \n",
       "3                    73.96               17.71          11.98      39.58   \n",
       "7                    96.88               63.02          62.50      90.62   \n",
       "5                    94.79               59.38          60.94      88.54   \n",
       "\n",
       "   Tutor_Tone  Human-likeness  \n",
       "5       17.19           94.79  \n",
       "1       92.19           87.50  \n",
       "7       37.50           89.62  \n",
       "6       36.98           93.23  \n",
       "0       39.58           95.31  \n",
       "4       21.88           68.23  \n",
       "3       17.71           93.23  \n",
       "8       16.15           90.62  \n",
       "2       19.79           96.35  \n",
       "2       19.79           93.75  \n",
       "4       16.67           97.40  \n",
       "6       15.10           95.31  \n",
       "8       54.72           37.74  \n",
       "0       90.00           35.00  \n",
       "1       47.40           52.08  \n",
       "3       45.31           52.08  \n",
       "7       57.81           98.96  \n",
       "5       54.69           96.35  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the annotation label to the desiderata point\n",
    "MRBenchv1_data_mapped = []\n",
    "for data in MRBenchv1_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        annotation = value['annotation']\n",
    "        new_annotation = {}\n",
    "        for k, v in annotation.items():\n",
    "            new_annotation[k] = map_annotation_label(k,v)\n",
    "        value['annotation_point'] = new_annotation\n",
    "    MRBenchv1_data_mapped.append(data)\n",
    "# print(MRBenchv1_data_mapped[0])\n",
    "# evaluate the desiderata point\n",
    "evaluation_result = evaluate_ordinary_desiderata(MRBenchv1_data)\n",
    "# print the evaluation result\n",
    "pd_result=print_evaluation_result(evaluation_result)\n",
    "\n",
    "import pandas as pd\n",
    "ss = pd.read_csv('../paper/paper_result.csv',sep='\\t')\n",
    "ss['Tutor'] = ss['Tutor'].apply(lambda x: x.replace(\"*\",\"\")+\"_paper\")\n",
    "concat_result = pd.concat([ss, pd_result], axis=0)\n",
    "concat_result = concat_result.sort_values(by='Tutor')\n",
    "concat_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81d6da",
   "metadata": {},
   "source": [
    "# Compute DAMR Score For LLAMA & MISTRAL inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa75bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_mistake_identification_result 192 89.32\n",
      "eval_mistake_location_result 192 76.82\n",
      "eval_revealing_of_the_answer_result 192 62.5\n",
      "eval_providing_guidance_result 192 50.52\n",
      "eval_actionability_result 192 41.93\n",
      "eval_coherence_result 192 88.02\n",
      "eval_tutor_tone_result 192 59.64\n",
      "eval_humanlikeness_result 192 77.86\n"
     ]
    }
   ],
   "source": [
    "#Compute DAMR Score For LLAMA & MISTRAL inference\n",
    "import json\n",
    "desiderata = {\n",
    "    \"eval_mistake_identification_result\": 1,  # Yes\n",
    "    \"eval_mistake_location_result\": 1,        # Yes\n",
    "    \"eval_revealing_of_the_answer_result\": 3,        # No\n",
    "    \"eval_providing_guidance_result\": 1,      # Yes\n",
    "    \"eval_actionability_result\": 1,           # Yes\n",
    "    \"eval_coherence_result\": 1,                # Yes\n",
    "    \"eval_tutor_tone_result\": 1,              # Encouraging\n",
    "    \"eval_humanlikeness_result\":1,               # Yes\n",
    "}\n",
    "\n",
    "# new_annotation\n",
    "def evaluate_ordinary_desiderata(MRBenchv1_eval_data,verbose=False):\n",
    "    from collections import defaultdict\n",
    "    evaluation_result = defaultdict(list)\n",
    "    for data in MRBenchv1_eval_data:\n",
    "        for sub, score in desiderata.items():\n",
    "            eval_score = data[sub]['number']\n",
    "            if eval_score is None:\n",
    "                print(data['conversation_id'] + data['Split'])\n",
    "                continue\n",
    "            if eval_score == score:\n",
    "                evaluation_result[sub].append(1)\n",
    "            elif eval_score == 2:\n",
    "                evaluation_result[sub].append(0.5)\n",
    "            else:\n",
    "                evaluation_result[sub].append(0)\n",
    "    for k, v in evaluation_result.items():\n",
    "        print(k,len(v), round(sum(v) / len(v) * 100, 2))\n",
    "\n",
    "llama_eval = json.load(open(\"../data/MRBench/MRBench_V1_Meta-Llama-3.1-8B-Instruct_llama_eval.json\"))\n",
    "\n",
    "evaluate_ordinary_desiderata(llama_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c913ee",
   "metadata": {},
   "source": [
    "# Compute AC Score\n",
    "\n",
    "-  Pearson/Spearman correlation between LLM as judge scores and human gold-standard ratings to measure reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e108575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def pearson_corr(y_true, y_pred, method=\"pearson\"):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    if method == \"pearson\":\n",
    "        r = pearsonr(y_true, y_pred)\n",
    "    elif method == \"spearman\":\n",
    "        r= spearmanr(y_true, y_pred)\n",
    "    return round(r.correlation, 3), round(r.pvalue, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "074d06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the annotation label to the desiderata point\n",
    "from collections import defaultdict\n",
    "MRBenchv1_data = json.load(open(\"../data/MRBench/MRBench_V1.json\"))\n",
    "MRBenchv1_model_point = defaultdict(dict)\n",
    "for data in MRBenchv1_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        annotation = value['annotation']\n",
    "        # new_annotation = {}\n",
    "        for k, v in annotation.items():\n",
    "            annotation_point = map_annotation_label(k,v)\n",
    "            if k not in MRBenchv1_model_point[key]:\n",
    "                MRBenchv1_model_point[key][k] = []\n",
    "            MRBenchv1_model_point[key][k].append([data['conversation_id']+data['Split'], annotation_point])\n",
    "temp=\"../data/MRBench/MRBench_V1_llama_eval.json\"\n",
    "from collections import defaultdict\n",
    "temp=json.load(open(temp))\n",
    "model_point = defaultdict(dict)\n",
    "for data in temp:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        annotation_eval = value['annotation_eval']\n",
    "        for k, v in annotation_eval.items():\n",
    "            if k not in model_point[key]:\n",
    "                model_point[key][k] = []\n",
    "            model_point[key][k].append([data['conversation_id']+data['Split'], v['number']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a810170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Mistake_Identification 191 191\n",
      "Gemini Mistake_Location 191 191\n",
      "Gemini Revealing_of_the_Answer 191 191\n",
      "Gemini Providing_Guidance 191 191\n",
      "Gemini Actionability 191 191\n",
      "Gemini humanlikeness 191 191\n",
      "Gemini Coherence 191 191\n",
      "Gemini Tutor_Tone 191 191\n",
      "Phi3 Mistake_Identification 191 191\n",
      "Phi3 Mistake_Location 191 191\n",
      "Phi3 Revealing_of_the_Answer 191 191\n",
      "Phi3 Providing_Guidance 191 191\n",
      "Phi3 Actionability 191 191\n",
      "Phi3 humanlikeness 191 191\n",
      "Phi3 Coherence 191 191\n",
      "Phi3 Tutor_Tone 191 191\n",
      "Llama318B Mistake_Identification 191 191\n",
      "Llama318B Mistake_Location 191 191\n",
      "Llama318B Revealing_of_the_Answer 191 191\n",
      "Llama318B Providing_Guidance 191 191\n",
      "Llama318B Actionability 191 191\n",
      "Llama318B humanlikeness 191 191\n",
      "Llama318B Coherence 191 191\n",
      "Llama318B Tutor_Tone 191 191\n",
      "Llama31405B Mistake_Identification 191 191\n",
      "Llama31405B Mistake_Location 191 191\n",
      "Llama31405B Revealing_of_the_Answer 191 191\n",
      "Llama31405B Providing_Guidance 191 191\n",
      "Llama31405B Actionability 191 191\n",
      "Llama31405B humanlikeness 191 191\n",
      "Llama31405B Coherence 191 191\n",
      "Llama31405B Tutor_Tone 191 191\n",
      "Mistral Mistake_Identification 191 191\n",
      "Mistral Mistake_Location 191 191\n",
      "Mistral Revealing_of_the_Answer 191 191\n",
      "Mistral Providing_Guidance 191 191\n",
      "Mistral Actionability 191 191\n",
      "Mistral humanlikeness 191 191\n",
      "Mistral Coherence 191 191\n",
      "Mistral Tutor_Tone 191 191\n",
      "Expert Mistake_Identification 191 191\n",
      "Expert Mistake_Location 191 191\n",
      "Expert Revealing_of_the_Answer 191 191\n",
      "Expert Providing_Guidance 191 191\n",
      "Expert Actionability 191 191\n",
      "Expert humanlikeness 191 191\n",
      "Expert Coherence 191 191\n",
      "Expert Tutor_Tone 191 191\n",
      "GPT4 Mistake_Identification 191 191\n",
      "GPT4 Mistake_Location 191 191\n",
      "GPT4 Revealing_of_the_Answer 191 191\n",
      "GPT4 Providing_Guidance 191 191\n",
      "GPT4 Actionability 191 191\n",
      "GPT4 humanlikeness 191 191\n",
      "GPT4 Coherence 191 191\n",
      "GPT4 Tutor_Tone 191 191\n",
      "Sonnet Mistake_Identification 191 191\n",
      "Sonnet Mistake_Location 191 191\n",
      "Sonnet Revealing_of_the_Answer 191 191\n",
      "Sonnet Providing_Guidance 191 191\n",
      "Sonnet Actionability 191 191\n",
      "Sonnet humanlikeness 191 191\n",
      "Sonnet Coherence 191 191\n",
      "Sonnet Tutor_Tone 191 191\n",
      "Novice Mistake_Identification 52 52\n",
      "Novice Mistake_Location 52 52\n",
      "Novice Revealing_of_the_Answer 52 52\n",
      "Novice Providing_Guidance 52 52\n",
      "Novice Actionability 52 52\n",
      "Novice humanlikeness 52 52\n",
      "Novice Coherence 52 52\n",
      "Novice Tutor_Tone 52 52\n",
      "Index(['index', 'Mistake_Identification', 'Mistake_Location',\n",
      "       'Revealing_of_the_Answer', 'Providing_Guidance', 'Actionability',\n",
      "       'humanlikeness', 'Coherence', 'Tutor_Tone'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tutor</th>\n",
       "      <th>Mistake_Identification</th>\n",
       "      <th>Mistake_Location</th>\n",
       "      <th>Revealing_of_the_Answer</th>\n",
       "      <th>Providing_Guidance</th>\n",
       "      <th>Actionability</th>\n",
       "      <th>Human-likeness</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Tutor_Tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phi3</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama318B</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expert</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT4</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sonnet</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Novice</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.780</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tutor  Mistake_Identification  Mistake_Location  \\\n",
       "0       Gemini                   0.077             0.042   \n",
       "1         Phi3                   0.579             0.492   \n",
       "2    Llama318B                   0.143             0.081   \n",
       "3  Llama31405B                   0.009            -0.113   \n",
       "4      Mistral                   0.238            -0.047   \n",
       "5       Expert                   0.053             0.096   \n",
       "6         GPT4                   0.393             0.144   \n",
       "7       Sonnet                   0.080             0.087   \n",
       "8       Novice                   0.637             0.563   \n",
       "\n",
       "   Revealing_of_the_Answer  Providing_Guidance  Actionability  Human-likeness  \\\n",
       "0                    0.368               0.061          0.026           0.012   \n",
       "1                    0.301               0.570          0.260           0.215   \n",
       "2                    0.243               0.138          0.062           0.083   \n",
       "3                    0.450              -0.046          0.129           0.056   \n",
       "4                    0.437               0.042          0.113           0.052   \n",
       "5                    0.235               0.064          0.082           0.012   \n",
       "6                    0.552               0.202          0.174           0.136   \n",
       "7                    0.509               0.102          0.213          -0.077   \n",
       "8                    0.646               0.780         -0.065           0.009   \n",
       "\n",
       "   Coherence  Tutor_Tone  \n",
       "0      0.009       0.493  \n",
       "1      0.483       0.566  \n",
       "2     -0.017       0.389  \n",
       "3     -0.038       0.316  \n",
       "4      0.076       0.337  \n",
       "5      0.106       0.354  \n",
       "6      0.106       0.395  \n",
       "7      0.236      -0.022  \n",
       "8      0.237       0.609  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result= defaultdict(dict)\n",
    "for k, v in MRBenchv1_model_point.items():\n",
    "    for kk, vv in v.items():\n",
    "        # print(k,kk)\n",
    "        # if kk == \"Mistake_Identification\":\n",
    "        compare_model_result = model_point[k][kk.lower()]\n",
    "        group1 = pd.DataFrame(vv, columns=['conversation_id', 'annotation_point'])\n",
    "        group2 = pd.DataFrame(compare_model_result, columns=['conversation_id', 'annotation_point'])\n",
    "        merge_result = pd.merge(group1, group2, on='conversation_id', how='inner')\n",
    "        merge_result = merge_result.dropna().drop_duplicates(subset=['conversation_id'])\n",
    "        y_true = merge_result['annotation_point_x']\n",
    "        y_pred = merge_result['annotation_point_y']\n",
    "        print(k,kk,len(y_true), len(y_pred))\n",
    "        # print(y_true.values, y_pred.values)\n",
    "        p_corr, p_pvalue = pearson_corr(y_true, y_pred, method=\"pearson\")\n",
    "        s_corr, s_pvalue = pearson_corr(y_true, y_pred, method=\"spearman\")\n",
    "        # result[kk][k] = [p_corr, p_pvalue, s_corr, s_pvalue, len(merge_result)]\n",
    "        result[kk][k] = p_corr\n",
    "        # break\n",
    "\n",
    "result = pd.DataFrame(result).reset_index()\n",
    "print(result.columns)\n",
    "result = result.rename(columns={'index': 'Tutor','humanlikeness': 'Human-likeness'})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25bb8cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tutor</th>\n",
       "      <th>Mistake_Identification</th>\n",
       "      <th>Mistake_Location</th>\n",
       "      <th>Revealing_of_the_Answer</th>\n",
       "      <th>Providing_Guidance</th>\n",
       "      <th>Actionability</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Tutor_Tone</th>\n",
       "      <th>Human-likeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expert</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Expert_paper</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT-4_paper</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT4</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini_paper</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama-3.1-405B_paper</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-3.1-8B_paper</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama318B</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mistral_paper</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Novice</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.780</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Novice_paper</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phi3</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phi3_paper</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sonnet</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sonnet_paper</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tutor  Mistake_Identification  Mistake_Location  \\\n",
       "5                Expert                   0.053             0.096   \n",
       "1          Expert_paper                  -0.010            -0.250   \n",
       "7           GPT-4_paper                  -0.070             0.010   \n",
       "6                  GPT4                   0.393             0.144   \n",
       "0                Gemini                   0.077             0.042   \n",
       "4          Gemini_paper                   0.020             0.090   \n",
       "8  Llama-3.1-405B_paper                  -0.030            -0.080   \n",
       "3    Llama-3.1-8B_paper                  -0.120            -0.370   \n",
       "3           Llama31405B                   0.009            -0.113   \n",
       "2             Llama318B                   0.143             0.081   \n",
       "4               Mistral                   0.238            -0.047   \n",
       "6         Mistral_paper                  -0.060            -0.110   \n",
       "8                Novice                   0.637             0.563   \n",
       "0          Novice_paper                  -0.370             0.090   \n",
       "1                  Phi3                   0.579             0.492   \n",
       "2            Phi3_paper                  -0.670            -0.580   \n",
       "7                Sonnet                   0.080             0.087   \n",
       "5          Sonnet_paper                  -0.110            -0.120   \n",
       "\n",
       "   Revealing_of_the_Answer  Providing_Guidance  Actionability  Coherence  \\\n",
       "5                    0.235               0.064          0.082      0.106   \n",
       "1                   -0.130              -0.190         -0.080     -0.110   \n",
       "7                   -0.200              -0.210          0.020     -0.020   \n",
       "6                    0.552               0.202          0.174      0.106   \n",
       "0                    0.368               0.061          0.026      0.009   \n",
       "4                   -0.060              -0.160         -0.120     -0.070   \n",
       "8                   -0.050              -0.050          0.000      0.060   \n",
       "3                   -0.170               0.040         -0.070     -0.160   \n",
       "3                    0.450              -0.046          0.129     -0.038   \n",
       "2                    0.243               0.138          0.062     -0.017   \n",
       "4                    0.437               0.042          0.113      0.076   \n",
       "6                   -0.100              -0.230         -0.150     -0.200   \n",
       "8                    0.646               0.780         -0.065      0.237   \n",
       "0                   -0.560              -0.720          0.150     -0.150   \n",
       "1                    0.301               0.570          0.260      0.483   \n",
       "2                   -0.510              -0.510         -0.460     -0.330   \n",
       "7                    0.509               0.102          0.213      0.236   \n",
       "5                   -0.210              -0.110         -0.220     -0.080   \n",
       "\n",
       "   Tutor_Tone  Human-likeness  \n",
       "5       0.354           0.012  \n",
       "1      -0.400           0.010  \n",
       "7      -0.110           0.080  \n",
       "6       0.395           0.136  \n",
       "0       0.493           0.012  \n",
       "4      -0.240           0.070  \n",
       "8      -0.130           0.110  \n",
       "3      -0.290           0.110  \n",
       "3       0.316           0.056  \n",
       "2       0.389           0.083  \n",
       "4       0.337           0.052  \n",
       "6      -0.190           0.060  \n",
       "8       0.609           0.009  \n",
       "0      -0.710           0.180  \n",
       "1       0.566           0.215  \n",
       "2      -0.620           0.030  \n",
       "7      -0.022          -0.077  \n",
       "5      -0.200           0.070  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ac_llama = pd.read_csv('../paper/paper_ac_llama.csv',sep='\\t')\n",
    "paper_ac_llama['Tutor'] = paper_ac_llama['Tutor'].apply(lambda x: x.replace(\"*\",\"\")+\"_paper\")\n",
    "paper_ac_llama = paper_ac_llama.sort_values(by='Tutor')\n",
    "paper_ac_llama\n",
    "concat_result = pd.concat([paper_ac_llama, result], axis=0)\n",
    "concat_result = concat_result.sort_values(by='Tutor')\n",
    "# add diff row to concat_result\n",
    "# LLama - LLama_paper\n",
    "concat_result.to_csv('../data/concat_ac.result',sep='\\t',index=False)\n",
    "concat_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbe919",
   "metadata": {},
   "source": [
    "# Note\n",
    "DAMR: We can observe that there are differences in the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
